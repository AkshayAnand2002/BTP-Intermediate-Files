{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install opendatasets --upgrade --quiet\n",
        "!pip install kaggle --quiet\n",
        "import opendatasets as od\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "dataset_url = 'https://www.kaggle.com/datasets/nilesh789/eurosat-rgb'\n",
        "od.download(dataset_url)\n",
        "\n",
        "# Set the path to the image folder\n",
        "data_path = '/content/eurosat-rgb/2750'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IokRrBGCfOP",
        "outputId": "0d61baae-349d-4ff5-84d8-b935fb16e05b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./eurosat-rgb\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Activation, Dropout, GlobalAveragePooling2D, BatchNormalization, ZeroPadding2D, AveragePooling2D, MaxPooling2D, Conv2D\n",
        "from keras.models import Model\n",
        "import keras.backend as K"
      ],
      "metadata": {
        "id": "vAQ4m7JuCpvp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/eurosat-rgb/2750'\n",
        "# Initialize image data and labels\n",
        "image_data = []\n",
        "labels = []\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load the image dataset\n",
        "for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".jpg\"):\n",
        "            # Read the image\n",
        "            image = cv2.imread(os.path.join(root, file))\n",
        "            # image = cv2.resize(image, (128, 128))\n",
        "            image = cv2.resize(image, (64, 64))\n",
        "            image = img_to_array(image)\n",
        "            image_data.append(image)\n",
        "\n",
        "            # Use the folder name as the label\n",
        "            label = root.split(os.path.sep)[-1]\n",
        "            labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays and normalize the images\n",
        "image_data = np.array(image_data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(x_train)}, Testing samples: {len(x_test)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH8qUCcACz4l",
        "outputId": "3bc768a9-110d-4fc9-f1a3-fd0c69071253"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 21600, Testing samples: 5400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def student_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
        "    nb_dense_block = 3  # Fewer dense blocks than the teacher model\n",
        "    growth_rate = 16  # Smaller growth rate\n",
        "    nb_filter = 32  # Fewer filters\n",
        "    return densenet121_model(img_rows=img_rows, img_cols=img_cols, color_type=color_type,\n",
        "                             nb_dense_block=nb_dense_block, growth_rate=growth_rate,\n",
        "                             nb_filter=nb_filter, num_classes=num_classes)\n",
        "\n",
        "def distillation_loss(y_true, y_pred, teacher_pred, temperature=3.0, alpha=0.1):\n",
        "    \"\"\"\n",
        "    Compute the distillation loss combining both:\n",
        "    - Soft target loss (KL divergence)\n",
        "    - Hard target loss (standard cross-entropy)\n",
        "    \"\"\"\n",
        "    soft_labels = K.softmax(teacher_pred / temperature)\n",
        "    soft_student = K.softmax(y_pred / temperature)\n",
        "\n",
        "    # KL divergence for the soft labels\n",
        "    distillation_loss = K.mean(K.sum(soft_labels * K.log(soft_labels / (soft_student + 1e-6)), axis=-1))\n",
        "\n",
        "    # Standard cross-entropy loss\n",
        "    standard_loss = K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Weighted sum of the losses\n",
        "    return alpha * distillation_loss + (1. - alpha) * standard_loss\n"
      ],
      "metadata": {
        "id": "Fgv79medC7ac"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Define DenseNet-121 model and helper functions\n",
        "from keras.layers import Concatenate, Conv2D, Activation, BatchNormalization, Dropout, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, Dense, MaxPooling2D, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "def densenet121_model(img_rows, img_cols, color_type=1, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.5, dropout_rate=0.0, weight_decay=1e-4, num_classes=None):\n",
        "    global concat_axis\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
        "    concat_axis = 3\n",
        "\n",
        "    nb_filter = 64\n",
        "    nb_layers = [6, 12, 24, 16]\n",
        "\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(img_input)\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "        x = transition_block(x, stage, nb_filter, dropout_rate=dropout_rate)\n",
        "        nb_filter = int(nb_filter)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x_fc = GlobalAveragePooling2D()(x)\n",
        "    x_fc = Dense(1000)(x_fc)\n",
        "    x_fc = Activation('softmax')(x_fc)\n",
        "\n",
        "    model = Model(img_input, x_fc)\n",
        "\n",
        "    x_newfc = GlobalAveragePooling2D()(x)\n",
        "    x_newfc = Dense(num_classes)(x_newfc)\n",
        "    x_newfc = Activation('softmax')(x_newfc)\n",
        "\n",
        "    model = Model(img_input, x_newfc)\n",
        "\n",
        "    sgd = SGD(learning_rate=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None):\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def transition_block(x, stage, nb_filter, dropout_rate=None):\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(int(nb_filter), (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, grow_nb_filters=True):\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i + 1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate)\n",
        "        concat_feat = Concatenate(axis=concat_axis)([concat_feat, x])\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n"
      ],
      "metadata": {
        "id": "F3bX_35bEuxd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Build the teacher and student models\n",
        "teacher = densenet121_model(img_rows=64, img_cols=64, color_type=3, num_classes=10)\n",
        "student = student_model(img_rows=64, img_cols=64, color_type=3, num_classes=10)\n",
        "\n",
        "# # Teacher model should be pre-trained (for simplicity, using it as untrained here)\n",
        "# for layer in teacher.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# # Compile the student model with the distillation loss\n",
        "# student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "#                 loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher.predict(x_train)),\n",
        "#                 metrics=['accuracy'])\n",
        "\n",
        "# # Train the student model\n",
        "# student.fit(x_train, y_train,\n",
        "#             batch_size=16,\n",
        "#             epochs=5,\n",
        "#             validation_data=(x_test, y_test),\n",
        "#             shuffle=True)\n",
        "# Precompute teacher's predictions for the training set\n",
        "teacher_preds = teacher.predict(x_train)\n",
        "\n",
        "# Define distillation loss function\n",
        "# def distillation_loss(y_true, y_pred, teacher_preds, temperature=3):\n",
        "#     y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "#     y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "#     teacher_preds = tf.keras.activations.softmax(teacher_preds / temperature)\n",
        "\n",
        "#     # Cross-entropy between the student predictions and the teacher predictions\n",
        "#     return tf.keras.losses.categorical_crossentropy(teacher_preds, y_pred)\n",
        "\n",
        "def distillation_loss(y_true, y_pred, teacher_preds, temperature=3):\n",
        "    # Get the current batch size\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    # Use tf.gather to select the relevant teacher predictions for the current batch\n",
        "    teacher_batch_preds = tf.gather(teacher_preds, tf.range(batch_size))\n",
        "\n",
        "    y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "    y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "    teacher_batch_preds = tf.keras.activations.softmax(teacher_batch_preds / temperature)\n",
        "\n",
        "    # Cross-entropy between the student predictions and the teacher predictions\n",
        "    return tf.keras.losses.categorical_crossentropy(teacher_batch_preds, y_pred)\n",
        "\n",
        "\n",
        "# Compile the student model with the distillation loss (using precomputed teacher predictions)\n",
        "student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "                loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Train the student model\n",
        "student.fit(x_train, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=5,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XqIyeWVE0y0",
        "outputId": "a947d081-7bc1-471e-953f-535651f0601a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m675/675\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 353ms/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 332ms/step - accuracy: 0.0840 - loss: 2.3029 - val_accuracy: 0.0833 - val_loss: 2.3028\n",
            "Epoch 2/5\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 354ms/step - accuracy: 0.0817 - loss: 2.3027 - val_accuracy: 0.0789 - val_loss: 2.3027\n",
            "Epoch 3/5\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 343ms/step - accuracy: 0.0796 - loss: 2.3027 - val_accuracy: 0.0783 - val_loss: 2.3027\n",
            "Epoch 4/5\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 343ms/step - accuracy: 0.0841 - loss: 2.3027 - val_accuracy: 0.0961 - val_loss: 2.3027\n",
            "Epoch 5/5\n",
            "\u001b[1m1350/1350\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 343ms/step - accuracy: 0.0865 - loss: 2.3027 - val_accuracy: 0.1043 - val_loss: 2.3027\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ee5efcb5c90>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Evaluate the trained student model\n",
        "score = student.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test accuracy: {score[1] * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vILBID5nE4hh",
        "outputId": "4c0e60f9-7e28-4df7-9657-05ea4991d2dd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 10.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teacher_score = teacher.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Teacher test accuracy: {teacher_score[1] * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d3_lHqbIgct",
        "outputId": "6b4fc8b2-7d8e-4be1-9d69-cccb29af9611"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher test accuracy: 12.26%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Evaluate and print additional metrics for teacher and student models\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    # Predict the labels\n",
        "    start_time = time.time()\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Get the predicted classes\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, F1 score\n",
        "    report = classification_report(y_true, y_pred, target_names=lb.classes_)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(report)\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "    return y_pred_probs\n",
        "\n",
        "print(\"=== Teacher Model Evaluation ===\")\n",
        "teacher_pred_probs = evaluate_model(teacher, x_test, y_test)\n",
        "\n",
        "print(\"\\n=== Student Model Evaluation ===\")\n",
        "student_pred_probs = evaluate_model(student, x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLWB8E7tIiTr",
        "outputId": "41d62ba1-353c-43af-c09c-c58380c01830"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Teacher Model Evaluation ===\n",
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 369ms/step\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.00      0.00      0.00       572\n",
            "              Forest       0.00      0.00      0.00       610\n",
            "HerbaceousVegetation       0.00      0.00      0.00       599\n",
            "             Highway       0.00      0.00      0.00       486\n",
            "          Industrial       0.00      0.00      0.00       482\n",
            "             Pasture       0.33      0.26      0.29       425\n",
            "       PermanentCrop       0.00      0.00      0.00       505\n",
            "         Residential       0.13      0.80      0.22       632\n",
            "               River       0.04      0.09      0.06       507\n",
            "             SeaLake       0.00      0.00      0.00       582\n",
            "\n",
            "            accuracy                           0.12      5400\n",
            "           macro avg       0.05      0.12      0.06      5400\n",
            "        weighted avg       0.04      0.12      0.05      5400\n",
            "\n",
            "Inference Time: 63.0206 seconds\n",
            "\n",
            "=== Student Model Evaluation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m169/169\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 141ms/step\n",
            "                      precision    recall  f1-score   support\n",
            "\n",
            "          AnnualCrop       0.05      0.06      0.06       572\n",
            "              Forest       0.27      0.13      0.17       610\n",
            "HerbaceousVegetation       0.02      0.00      0.00       599\n",
            "             Highway       0.05      0.01      0.02       486\n",
            "          Industrial       0.12      0.13      0.12       482\n",
            "             Pasture       0.06      0.07      0.06       425\n",
            "       PermanentCrop       0.11      0.01      0.02       505\n",
            "         Residential       0.09      0.08      0.08       632\n",
            "               River       0.09      0.36      0.15       507\n",
            "             SeaLake       0.18      0.20      0.19       582\n",
            "\n",
            "            accuracy                           0.10      5400\n",
            "           macro avg       0.10      0.10      0.09      5400\n",
            "        weighted avg       0.11      0.10      0.09      5400\n",
            "\n",
            "Inference Time: 41.4499 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "HpyOwupUIkYw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}