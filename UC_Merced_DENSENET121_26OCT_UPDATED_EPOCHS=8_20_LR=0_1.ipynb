{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQoLOgZ7Vz_r",
        "outputId": "8f9b37db-2122-4313-dc15-b9a1efd7b945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: akshayanand2002\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/abdulhasibuddin/uc-merced-land-use-dataset\n",
            "Downloading uc-merced-land-use-dataset.zip to ./uc-merced-land-use-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 317M/317M [00:11<00:00, 28.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries\n",
        "!pip install opendatasets --upgrade --quiet\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# Import Kaggle and OpenDatasets to download datasets from Kaggle\n",
        "import opendatasets as od\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "dataset_url = 'https://www.kaggle.com/datasets/abdulhasibuddin/uc-merced-land-use-dataset'\n",
        "od.download(dataset_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xy1TVwQV0sj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Activation, Dropout, GlobalAveragePooling2D, BatchNormalization, ZeroPadding2D, AveragePooling2D, MaxPooling2D, Conv2D\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsZPZyA2V1_1",
        "outputId": "12d1d40f-8c44-40b9-ca62-a628971eaa8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1680, Testing samples: 420\n"
          ]
        }
      ],
      "source": [
        "# Set the path to the image folder\n",
        "data_path = 'uc-merced-land-use-dataset/UCMerced_LandUse/Images'\n",
        "\n",
        "# Initialize image data and labels\n",
        "image_data = []\n",
        "labels = []\n",
        "\n",
        "# Load the image dataset\n",
        "for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".tif\"):\n",
        "            # Read the image\n",
        "            image = cv2.imread(os.path.join(root, file))\n",
        "            image = cv2.resize(image, (128, 128))\n",
        "            image = img_to_array(image)\n",
        "            image_data.append(image)\n",
        "\n",
        "            # Use the folder name as the label\n",
        "            label = root.split(os.path.sep)[-1]\n",
        "            labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays and normalize the images\n",
        "image_data = np.array(image_data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(x_train)}, Testing samples: {len(x_test)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "fmYie7ZyV4nn",
        "outputId": "008568cc-2f50-47ae-ba1b-6fb902472a21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef distillation_loss(y_true, y_pred, teacher_pred, temperature=3.0, alpha=0.1):\\n    \"\"\"\\n    Compute the distillation loss combining both:\\n    - Soft target loss (KL divergence)\\n    - Hard target loss (standard cross-entropy)\\n    \"\"\"\\n    soft_labels = K.softmax(teacher_pred / temperature)\\n    soft_student = K.softmax(y_pred / temperature)\\n\\n    # KL divergence for the soft labels\\n    distillation_loss = K.mean(K.sum(soft_labels * K.log(soft_labels / (soft_student + 1e-6)), axis=-1))\\n\\n    # Standard cross-entropy loss\\n    standard_loss = K.categorical_crossentropy(y_true, y_pred)\\n\\n    # Weighted sum of the losses\\n    return alpha * distillation_loss + (1. - alpha) * standard_loss\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def student_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
        "    nb_dense_block = 3  # Fewer dense blocks than the teacher model\n",
        "    growth_rate = 32  # Smaller growth rate\n",
        "    nb_filter = 32  # Fewer filters\n",
        "    return densenet121_model(img_rows=img_rows, img_cols=img_cols, color_type=color_type,\n",
        "                             nb_dense_block=nb_dense_block, growth_rate=growth_rate,\n",
        "                             nb_filter=nb_filter, num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo3AgO1_X_Fw"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Concatenate, Conv2D, Activation, BatchNormalization, Dropout, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, Dense, MaxPooling2D, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "def densenet121_model(img_rows, img_cols, color_type=1, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.5, dropout_rate=0.0, weight_decay=1e-4, num_classes=None):\n",
        "    global concat_axis\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
        "    concat_axis = 3\n",
        "\n",
        "    nb_filter = 64\n",
        "    nb_layers = [6, 12, 24, 16]\n",
        "\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(img_input)\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "        x = transition_block(x, stage, nb_filter, dropout_rate=dropout_rate)\n",
        "        nb_filter = int(nb_filter)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x_fc = GlobalAveragePooling2D()(x)\n",
        "    x_fc = Dense(1000)(x_fc)\n",
        "    x_fc = Activation('softmax')(x_fc)\n",
        "\n",
        "    model = Model(img_input, x_fc)\n",
        "\n",
        "    x_newfc = GlobalAveragePooling2D()(x)\n",
        "    x_newfc = Dense(num_classes)(x_newfc)\n",
        "    x_newfc = Activation('softmax')(x_newfc)\n",
        "\n",
        "    model = Model(img_input, x_newfc)\n",
        "\n",
        "    sgd = SGD(learning_rate=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None):\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def transition_block(x, stage, nb_filter, dropout_rate=None):\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(int(nb_filter), (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, grow_nb_filters=True):\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i + 1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate)\n",
        "        concat_feat = Concatenate(axis=concat_axis)([concat_feat, x])\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkze25r2V7aE",
        "outputId": "5a39417f-1382-4066-a00e-a5c3fa6ce7d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 9s/step - accuracy: 0.2119 - loss: 2.7510 - val_accuracy: 0.0667 - val_loss: 3.8740\n",
            "Epoch 2/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 9s/step - accuracy: 0.4523 - loss: 1.9220 - val_accuracy: 0.1357 - val_loss: 3.3072\n",
            "Epoch 3/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m483s\u001b[0m 9s/step - accuracy: 0.4932 - loss: 1.6031 - val_accuracy: 0.1190 - val_loss: 3.6098\n",
            "Epoch 4/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 9s/step - accuracy: 0.5838 - loss: 1.3487 - val_accuracy: 0.2333 - val_loss: 2.8818\n",
            "Epoch 5/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 9s/step - accuracy: 0.6919 - loss: 0.9659 - val_accuracy: 0.3548 - val_loss: 2.3230\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step\n",
            "Epoch 1/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 7s/step - accuracy: 0.0298 - loss: 3.0446 - val_accuracy: 0.0619 - val_loss: 3.0445\n",
            "Epoch 2/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 7s/step - accuracy: 0.0312 - loss: 3.0446 - val_accuracy: 0.0619 - val_loss: 3.0445\n",
            "Epoch 3/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 7s/step - accuracy: 0.0345 - loss: 3.0446 - val_accuracy: 0.0619 - val_loss: 3.0445\n",
            "Epoch 4/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 7s/step - accuracy: 0.0348 - loss: 3.0446 - val_accuracy: 0.0619 - val_loss: 3.0445\n",
            "Epoch 5/5\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 7s/step - accuracy: 0.0303 - loss: 3.0446 - val_accuracy: 0.0619 - val_loss: 3.0445\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x77fffdc79180>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Assuming densenet121_model and student_model functions are already defined\n",
        "# Replace with appropriate model definitions if necessary\n",
        "\n",
        "# Build the teacher and student models\n",
        "teacher = densenet121_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)\n",
        "student = student_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)\n",
        "\n",
        "# Compile the teacher model (pre-training the teacher model if necessary)\n",
        "teacher.compile(optimizer=SGD(learning_rate=1e-2, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train teacher model (pretrain if needed)\n",
        "# Note: If teacher model is already pre-trained, you can load weights here instead of training\n",
        "teacher.fit(x_train, y_train,\n",
        "            batch_size=32,\n",
        "            epochs=5,  # Adjust epochs as needed for pre-training\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "# Precompute teacher's predictions for the training set\n",
        "teacher_preds = teacher.predict(x_train)\n",
        "\n",
        "# Define distillation loss function\n",
        "def distillation_loss(y_true, y_pred, teacher_preds, temperature=3.0, alpha=0.1):\n",
        "    \"\"\"\n",
        "    Compute the distillation loss combining both:\n",
        "    - Soft target loss (KL divergence)\n",
        "    - Hard target loss (standard cross-entropy)\n",
        "    \"\"\"\n",
        "    # Get the current batch size\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    # Use tf.gather to select the relevant teacher predictions for the current batch\n",
        "    teacher_batch_preds = tf.gather(teacher_preds, tf.range(batch_size))\n",
        "\n",
        "    # Softmax for soft labels and student predictions with temperature scaling\n",
        "    y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "    y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "    teacher_batch_preds = tf.keras.activations.softmax(teacher_batch_preds / temperature)\n",
        "\n",
        "    # Cross-entropy between the student predictions and the teacher predictions (soft targets)\n",
        "    soft_loss = tf.keras.losses.categorical_crossentropy(teacher_batch_preds, y_pred)\n",
        "\n",
        "    # Standard cross-entropy loss with hard targets\n",
        "    hard_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Weighted sum of the distillation loss and the hard target loss\n",
        "    return alpha * soft_loss + (1. - alpha) * hard_loss\n",
        "\n",
        "# # Compile the student model with the distillation loss (using precomputed teacher predictions)\n",
        "# student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "#                 loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds),\n",
        "#                 metrics=['accuracy'])\n",
        "\n",
        "# # Train the student model\n",
        "# student.fit(x_train, y_train,\n",
        "#             batch_size=32,\n",
        "#             epochs=15,\n",
        "#             validation_data=(x_test, y_test))\n",
        "student.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds), metrics=['accuracy'])\n",
        "student.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_test, y_test), callbacks=[LearningRateScheduler(lr_scheduler)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VipkPiCJV8_J",
        "outputId": "69a09b75-8529-4745-ff48-ecd6458bb416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 6.19%\n"
          ]
        }
      ],
      "source": [
        " # Evaluate the trained student model\n",
        "score = student.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test accuracy: {score[1] * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYnfWU1GWBnT",
        "outputId": "9883340f-560c-4d1c-e18d-8eada2a9ff00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher test accuracy: 35.48%\n"
          ]
        }
      ],
      "source": [
        "teacher_score = teacher.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Teacher test accuracy: {teacher_score[1] * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poCzU3RBkdJs",
        "outputId": "67a3bbb0-5aad-4d0e-b7b6-8e2c1e71e2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Teacher Model Evaluation ===\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     agricultural       0.00      0.00      0.00        18\n",
            "         airplane       1.00      0.25      0.40        20\n",
            "  baseballdiamond       0.71      0.19      0.30        26\n",
            "            beach       1.00      0.36      0.53        22\n",
            "        buildings       1.00      0.19      0.32        21\n",
            "        chaparral       0.54      0.93      0.68        27\n",
            " denseresidential       0.29      0.90      0.43        20\n",
            "           forest       0.31      1.00      0.48        17\n",
            "          freeway       0.36      0.43      0.39        23\n",
            "       golfcourse       0.31      0.22      0.26        18\n",
            "           harbor       0.86      0.71      0.77        17\n",
            "     intersection       0.00      0.00      0.00        14\n",
            "mediumresidential       0.20      0.70      0.31        23\n",
            "   mobilehomepark       1.00      0.13      0.24        15\n",
            "         overpass       0.00      0.00      0.00        16\n",
            "       parkinglot       1.00      0.17      0.29        18\n",
            "            river       0.27      0.12      0.17        24\n",
            "           runway       0.62      0.28      0.38        18\n",
            "sparseresidential       0.16      0.30      0.21        20\n",
            "     storagetanks       1.00      0.04      0.07        26\n",
            "      tenniscourt       0.19      0.29      0.23        17\n",
            "\n",
            "         accuracy                           0.35       420\n",
            "        macro avg       0.52      0.34      0.31       420\n",
            "     weighted avg       0.53      0.35      0.32       420\n",
            "\n",
            "Inference Time: 23.3918 seconds\n",
            "\n",
            "=== Student Model Evaluation ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     agricultural       0.00      0.00      0.00        18\n",
            "         airplane       0.00      0.00      0.00        20\n",
            "  baseballdiamond       0.00      0.00      0.00        26\n",
            "            beach       0.00      0.00      0.00        22\n",
            "        buildings       0.00      0.00      0.00        21\n",
            "        chaparral       0.00      0.00      0.00        27\n",
            " denseresidential       0.00      0.00      0.00        20\n",
            "           forest       0.00      0.00      0.00        17\n",
            "          freeway       0.00      0.00      0.00        23\n",
            "       golfcourse       0.00      0.00      0.00        18\n",
            "           harbor       0.00      0.00      0.00        17\n",
            "     intersection       0.00      0.00      0.00        14\n",
            "mediumresidential       0.00      0.00      0.00        23\n",
            "   mobilehomepark       0.00      0.00      0.00        15\n",
            "         overpass       0.00      0.00      0.00        16\n",
            "       parkinglot       0.00      0.00      0.00        18\n",
            "            river       0.00      0.00      0.00        24\n",
            "           runway       0.00      0.00      0.00        18\n",
            "sparseresidential       0.00      0.00      0.00        20\n",
            "     storagetanks       0.06      1.00      0.12        26\n",
            "      tenniscourt       0.00      0.00      0.00        17\n",
            "\n",
            "         accuracy                           0.06       420\n",
            "        macro avg       0.00      0.05      0.01       420\n",
            "     weighted avg       0.00      0.06      0.01       420\n",
            "\n",
            "Inference Time: 41.1687 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Cell: Evaluate and print additional metrics for teacher and student models\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    # Predict the labels\n",
        "    start_time = time.time()\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Get the predicted classes\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, F1 score\n",
        "    report = classification_report(y_true, y_pred, target_names=lb.classes_)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(report)\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "    return y_pred_probs\n",
        "\n",
        "print(\"=== Teacher Model Evaluation ===\")\n",
        "teacher_pred_probs = evaluate_model(teacher, x_test, y_test)\n",
        "\n",
        "print(\"\\n=== Student Model Evaluation ===\")\n",
        "student_pred_probs = evaluate_model(student, x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJsQnbwR6Rj3"
      },
      "outputs": [],
      "source": [
        "########------------------#####################--------##########"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install opendatasets --upgrade --quiet\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# Import Kaggle and OpenDatasets to download datasets from Kaggle\n",
        "import opendatasets as od\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "dataset_url = 'https://www.kaggle.com/datasets/abdulhasibuddin/uc-merced-land-use-dataset'\n",
        "od.download(dataset_url)\n",
        "\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Activation, Dropout, GlobalAveragePooling2D, BatchNormalization, ZeroPadding2D, AveragePooling2D, MaxPooling2D, Conv2D\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n",
        "\n",
        "# Set the path to the image folder\n",
        "data_path = 'uc-merced-land-use-dataset/UCMerced_LandUse/Images'\n",
        "\n",
        "# Initialize image data and labels\n",
        "image_data = []\n",
        "labels = []\n",
        "\n",
        "# Load the image dataset\n",
        "for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".tif\"):\n",
        "            # Read the image\n",
        "            image = cv2.imread(os.path.join(root, file))\n",
        "            image = cv2.resize(image, (128, 128))\n",
        "            image = img_to_array(image)\n",
        "            image_data.append(image)\n",
        "\n",
        "            # Use the folder name as the label\n",
        "            label = root.split(os.path.sep)[-1]\n",
        "            labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays and normalize the images\n",
        "image_data = np.array(image_data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(x_train)}, Testing samples: {len(x_test)}')\n",
        "\n",
        "def student_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
        "    nb_dense_block = 3  # Fewer dense blocks than the teacher model\n",
        "    growth_rate = 32  # Smaller growth rate\n",
        "    nb_filter = 32  # Fewer filters\n",
        "    return densenet121_model(img_rows=img_rows, img_cols=img_cols, color_type=color_type,\n",
        "                             nb_dense_block=nb_dense_block, growth_rate=growth_rate,\n",
        "                             nb_filter=nb_filter, num_classes=num_classes)\n",
        "\n",
        "from keras.layers import Concatenate, Conv2D, Activation, BatchNormalization, Dropout, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, Dense, MaxPooling2D, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "def densenet121_model(img_rows, img_cols, color_type=1, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.5, dropout_rate=0.0, weight_decay=1e-4, num_classes=None):\n",
        "    global concat_axis\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
        "    concat_axis = 3\n",
        "\n",
        "    nb_filter = 64\n",
        "    nb_layers = [6, 12, 24, 16]\n",
        "\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(img_input)\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "        x = transition_block(x, stage, nb_filter, dropout_rate=dropout_rate)\n",
        "        nb_filter = int(nb_filter)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x_fc = GlobalAveragePooling2D()(x)\n",
        "    x_fc = Dense(1000)(x_fc)\n",
        "    x_fc = Activation('softmax')(x_fc)\n",
        "\n",
        "    model = Model(img_input, x_fc)\n",
        "\n",
        "    x_newfc = GlobalAveragePooling2D()(x)\n",
        "    x_newfc = Dense(num_classes)(x_newfc)\n",
        "    x_newfc = Activation('softmax')(x_newfc)\n",
        "\n",
        "    model = Model(img_input, x_newfc)\n",
        "\n",
        "    sgd = SGD(learning_rate=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None):\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def transition_block(x, stage, nb_filter, dropout_rate=None):\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(int(nb_filter), (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, grow_nb_filters=True):\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i + 1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate)\n",
        "        concat_feat = Concatenate(axis=concat_axis)([concat_feat, x])\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Build the teacher and student models\n",
        "teacher = densenet121_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)\n",
        "student = student_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)\n",
        "\n",
        "# Define a learning rate scheduler function\n",
        "# def lr_scheduler(epoch, lr):\n",
        "#     if epoch < 10:\n",
        "#         return lr\n",
        "#     else:\n",
        "#         return lr * tf.math.exp(-0.1)\n",
        "\n",
        "def lr_scheduler(epoch, lr):\n",
        "    return float(lr * tf.math.exp(-0.1)) if epoch >= 10 else float(lr)\n",
        "\n",
        "# Compile the teacher model (pre-training the teacher model if necessary)\n",
        "teacher.compile(optimizer=SGD(learning_rate=1e-2, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train teacher model with the learning rate scheduler\n",
        "teacher.fit(x_train, y_train,\n",
        "            batch_size=32,\n",
        "            epochs=8,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[LearningRateScheduler(lr_scheduler)])\n",
        "\n",
        "# Precompute teacher's predictions for the training set\n",
        "teacher_preds = teacher.predict(x_train)\n",
        "\n",
        "# Define distillation loss function\n",
        "def distillation_loss(y_true, y_pred, teacher_preds, temperature=3.0, alpha=0.1):\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "    teacher_batch_preds = tf.gather(teacher_preds, tf.range(batch_size))\n",
        "\n",
        "    y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "    y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "    teacher_batch_preds = tf.keras.activations.softmax(teacher_batch_preds / temperature)\n",
        "\n",
        "    soft_loss = tf.keras.losses.categorical_crossentropy(teacher_batch_preds, y_pred)\n",
        "    hard_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    return alpha * soft_loss + (1. - alpha) * hard_loss\n",
        "\n",
        "# # Compile the student model with the distillation loss\n",
        "# student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "#                 loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds),\n",
        "#                 metrics=['accuracy'])\n",
        "\n",
        "# # Train the student model with learning rate scheduler\n",
        "# student.fit(x_train, y_train,\n",
        "#             batch_size=32,\n",
        "#             epochs=5,\n",
        "#             validation_data=(x_test, y_test),\n",
        "#             callbacks=[LearningRateScheduler(lr_scheduler)])\n",
        "\n",
        "student.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds), metrics=['accuracy'])\n",
        "student.fit(x_train, y_train, batch_size=32, epochs=20, validation_data=(x_test, y_test), callbacks=[LearningRateScheduler(lr_scheduler)])\n",
        "\n",
        "\n",
        "# Evaluate the trained student model\n",
        "score = student.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test accuracy: {score[1] * 100:.2f}%')\n",
        "\n",
        "teacher_score = teacher.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Teacher test accuracy: {teacher_score[1] * 100:.2f}%')\n",
        "\n",
        "# Cell: Evaluate and print additional metrics for teacher and student models\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    # Predict the labels\n",
        "    start_time = time.time()\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Get the predicted classes\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, F1 score\n",
        "    report = classification_report(y_true, y_pred, target_names=lb.classes_)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(report)\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "    return y_pred_probs\n",
        "\n",
        "print(\"=== Teacher Model Evaluation ===\")\n",
        "teacher_pred_probs = evaluate_model(teacher, x_test, y_test)\n",
        "\n",
        "print(\"\\n=== Student Model Evaluation ===\")\n",
        "student_pred_probs = evaluate_model(student, x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKOIpSa8buh0",
        "outputId": "eb26d506-f18d-4ef5-881f-ed6ba3f75ff7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./uc-merced-land-use-dataset\" (use force=True to force download)\n",
            "Training samples: 1680, Testing samples: 420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 8s/step - accuracy: 0.2334 - loss: 2.6295 - val_accuracy: 0.0333 - val_loss: 3.4354 - learning_rate: 0.0100\n",
            "Epoch 2/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 8s/step - accuracy: 0.4586 - loss: 1.9227 - val_accuracy: 0.0929 - val_loss: 3.7359 - learning_rate: 0.0100\n",
            "Epoch 3/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 8s/step - accuracy: 0.5261 - loss: 1.5221 - val_accuracy: 0.1595 - val_loss: 2.9105 - learning_rate: 0.0100\n",
            "Epoch 4/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 8s/step - accuracy: 0.6261 - loss: 1.2318 - val_accuracy: 0.0952 - val_loss: 4.3204 - learning_rate: 0.0100\n",
            "Epoch 5/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 8s/step - accuracy: 0.6806 - loss: 0.9957 - val_accuracy: 0.1190 - val_loss: 5.2867 - learning_rate: 0.0100\n",
            "Epoch 6/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 8s/step - accuracy: 0.7678 - loss: 0.7517 - val_accuracy: 0.4333 - val_loss: 1.8996 - learning_rate: 0.0100\n",
            "Epoch 7/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m449s\u001b[0m 8s/step - accuracy: 0.8340 - loss: 0.5202 - val_accuracy: 0.4333 - val_loss: 2.2560 - learning_rate: 0.0100\n",
            "Epoch 8/8\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 8s/step - accuracy: 0.8292 - loss: 0.5049 - val_accuracy: 0.2929 - val_loss: 4.5175 - learning_rate: 0.0100\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 2s/step\n",
            "Epoch 1/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 6s/step - accuracy: 0.1886 - loss: 3.0444 - val_accuracy: 0.0810 - val_loss: 3.0445 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 6s/step - accuracy: 0.3207 - loss: 3.0442 - val_accuracy: 0.0429 - val_loss: 3.0455 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 6s/step - accuracy: 0.4428 - loss: 3.0439 - val_accuracy: 0.1048 - val_loss: 3.0452 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m385s\u001b[0m 6s/step - accuracy: 0.4309 - loss: 3.0439 - val_accuracy: 0.0952 - val_loss: 3.0464 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 6s/step - accuracy: 0.4881 - loss: 3.0437 - val_accuracy: 0.0976 - val_loss: 3.0453 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 6s/step - accuracy: 0.5717 - loss: 3.0435 - val_accuracy: 0.1000 - val_loss: 3.0456 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 6s/step - accuracy: 0.6272 - loss: 3.0433 - val_accuracy: 0.2429 - val_loss: 3.0447 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 6s/step - accuracy: 0.6238 - loss: 3.0433 - val_accuracy: 0.1429 - val_loss: 3.0456 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 6s/step - accuracy: 0.6598 - loss: 3.0432 - val_accuracy: 0.3143 - val_loss: 3.0446 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 6s/step - accuracy: 0.7327 - loss: 3.0430 - val_accuracy: 0.1571 - val_loss: 3.0458 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 6s/step - accuracy: 0.7464 - loss: 3.0430 - val_accuracy: 0.3476 - val_loss: 3.0444 - learning_rate: 9.0484e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 6s/step - accuracy: 0.7708 - loss: 3.0429 - val_accuracy: 0.4238 - val_loss: 3.0442 - learning_rate: 8.1873e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 6s/step - accuracy: 0.8263 - loss: 3.0427 - val_accuracy: 0.6190 - val_loss: 3.0434 - learning_rate: 7.4082e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 6s/step - accuracy: 0.8540 - loss: 3.0426 - val_accuracy: 0.4214 - val_loss: 3.0442 - learning_rate: 6.7032e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m335s\u001b[0m 6s/step - accuracy: 0.8666 - loss: 3.0425 - val_accuracy: 0.5500 - val_loss: 3.0436 - learning_rate: 6.0653e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 6s/step - accuracy: 0.8851 - loss: 3.0425 - val_accuracy: 0.5119 - val_loss: 3.0437 - learning_rate: 5.4881e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 6s/step - accuracy: 0.8987 - loss: 3.0424 - val_accuracy: 0.7143 - val_loss: 3.0431 - learning_rate: 4.9659e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 6s/step - accuracy: 0.9160 - loss: 3.0424 - val_accuracy: 0.4976 - val_loss: 3.0439 - learning_rate: 4.4933e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 6s/step - accuracy: 0.9311 - loss: 3.0423 - val_accuracy: 0.5857 - val_loss: 3.0435 - learning_rate: 4.0657e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 6s/step - accuracy: 0.9456 - loss: 3.0423 - val_accuracy: 0.5095 - val_loss: 3.0438 - learning_rate: 3.6788e-04\n",
            "Test accuracy: 50.95%\n",
            "Teacher test accuracy: 29.29%\n",
            "=== Teacher Model Evaluation ===\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     agricultural       1.00      0.04      0.08        23\n",
            "         airplane       0.55      0.60      0.57        20\n",
            "  baseballdiamond       0.86      0.27      0.41        22\n",
            "            beach       0.00      0.00      0.00        18\n",
            "        buildings       0.00      0.00      0.00        17\n",
            "        chaparral       0.33      0.07      0.12        14\n",
            " denseresidential       0.75      0.17      0.27        18\n",
            "           forest       0.62      0.28      0.38        18\n",
            "          freeway       0.11      0.29      0.16        21\n",
            "       golfcourse       0.14      0.04      0.06        26\n",
            "           harbor       1.00      0.42      0.59        24\n",
            "     intersection       0.32      0.33      0.32        18\n",
            "mediumresidential       0.62      0.37      0.47        27\n",
            "   mobilehomepark       0.48      0.82      0.61        17\n",
            "         overpass       0.67      0.24      0.35        17\n",
            "       parkinglot       1.00      0.13      0.24        15\n",
            "            river       0.35      0.42      0.39        26\n",
            "           runway       0.09      0.88      0.16        16\n",
            "sparseresidential       0.55      0.26      0.35        23\n",
            "     storagetanks       0.38      0.50      0.43        20\n",
            "      tenniscourt       1.00      0.05      0.10        20\n",
            "\n",
            "         accuracy                           0.29       420\n",
            "        macro avg       0.52      0.29      0.29       420\n",
            "     weighted avg       0.52      0.29      0.30       420\n",
            "\n",
            "Inference Time: 41.1647 seconds\n",
            "\n",
            "=== Student Model Evaluation ===\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     agricultural       0.92      0.52      0.67        23\n",
            "         airplane       1.00      0.60      0.75        20\n",
            "  baseballdiamond       0.89      0.36      0.52        22\n",
            "            beach       1.00      0.28      0.43        18\n",
            "        buildings       0.75      0.35      0.48        17\n",
            "        chaparral       0.30      0.93      0.46        14\n",
            " denseresidential       0.44      0.78      0.56        18\n",
            "           forest       0.92      0.67      0.77        18\n",
            "          freeway       0.31      0.71      0.43        21\n",
            "       golfcourse       0.83      0.19      0.31        26\n",
            "           harbor       1.00      0.79      0.88        24\n",
            "     intersection       0.17      0.83      0.29        18\n",
            "mediumresidential       0.83      0.19      0.30        27\n",
            "   mobilehomepark       1.00      0.18      0.30        17\n",
            "         overpass       0.73      0.47      0.57        17\n",
            "       parkinglot       0.75      0.80      0.77        15\n",
            "            river       0.51      0.81      0.63        26\n",
            "           runway       0.92      0.75      0.83        16\n",
            "sparseresidential       0.44      0.48      0.46        23\n",
            "     storagetanks       1.00      0.15      0.26        20\n",
            "      tenniscourt       0.50      0.15      0.23        20\n",
            "\n",
            "         accuracy                           0.51       420\n",
            "        macro avg       0.72      0.52      0.52       420\n",
            "     weighted avg       0.73      0.51      0.51       420\n",
            "\n",
            "Inference Time: 22.3711 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H9E_P_T0mcor"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}