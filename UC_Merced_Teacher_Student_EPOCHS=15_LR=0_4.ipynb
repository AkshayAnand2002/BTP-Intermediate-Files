{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQoLOgZ7Vz_r",
        "outputId": "8c9620e8-8c0e-4d15-df2d-2c255d175799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: akshayanand2002\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/abdulhasibuddin/uc-merced-land-use-dataset\n",
            "Downloading uc-merced-land-use-dataset.zip to ./uc-merced-land-use-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 317M/317M [00:02<00:00, 137MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Install the required libraries\n",
        "!pip install opendatasets --upgrade --quiet\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# Import Kaggle and OpenDatasets to download datasets from Kaggle\n",
        "import opendatasets as od\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "dataset_url = 'https://www.kaggle.com/datasets/abdulhasibuddin/uc-merced-land-use-dataset'\n",
        "od.download(dataset_url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_Xy1TVwQV0sj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Input, Dense, Activation, Dropout, GlobalAveragePooling2D, BatchNormalization, ZeroPadding2D, AveragePooling2D, MaxPooling2D, Conv2D\n",
        "from keras.models import Model\n",
        "import keras.backend as K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsZPZyA2V1_1",
        "outputId": "cedf8dda-af74-4a80-e8f5-278d1a48d8c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 1680, Testing samples: 420\n"
          ]
        }
      ],
      "source": [
        "# Set the path to the image folder\n",
        "data_path = 'uc-merced-land-use-dataset/UCMerced_LandUse/Images'\n",
        "\n",
        "# Initialize image data and labels\n",
        "image_data = []\n",
        "labels = []\n",
        "\n",
        "# Load the image dataset\n",
        "for root, dirs, files in os.walk(data_path):\n",
        "    for file in files:\n",
        "        if file.endswith(\".tif\"):\n",
        "            # Read the image\n",
        "            image = cv2.imread(os.path.join(root, file))\n",
        "            image = cv2.resize(image, (128, 128))\n",
        "            image = img_to_array(image)\n",
        "            image_data.append(image)\n",
        "\n",
        "            # Use the folder name as the label\n",
        "            label = root.split(os.path.sep)[-1]\n",
        "            labels.append(label)\n",
        "\n",
        "# Convert to numpy arrays and normalize the images\n",
        "image_data = np.array(image_data, dtype=\"float\") / 255.0\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Binarize the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f'Training samples: {len(x_train)}, Testing samples: {len(x_test)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "fmYie7ZyV4nn",
        "outputId": "d6b41f5e-4cc3-4814-e548-6e095415d035"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef distillation_loss(y_true, y_pred, teacher_pred, temperature=3.0, alpha=0.1):\\n    \"\"\"\\n    Compute the distillation loss combining both:\\n    - Soft target loss (KL divergence)\\n    - Hard target loss (standard cross-entropy)\\n    \"\"\"\\n    soft_labels = K.softmax(teacher_pred / temperature)\\n    soft_student = K.softmax(y_pred / temperature)\\n\\n    # KL divergence for the soft labels\\n    distillation_loss = K.mean(K.sum(soft_labels * K.log(soft_labels / (soft_student + 1e-6)), axis=-1))\\n\\n    # Standard cross-entropy loss\\n    standard_loss = K.categorical_crossentropy(y_true, y_pred)\\n\\n    # Weighted sum of the losses\\n    return alpha * distillation_loss + (1. - alpha) * standard_loss\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def student_model(img_rows, img_cols, color_type=1, num_classes=None):\n",
        "    nb_dense_block = 3  # Fewer dense blocks than the teacher model\n",
        "    growth_rate = 16  # Smaller growth rate\n",
        "    nb_filter = 32  # Fewer filters\n",
        "    return densenet121_model(img_rows=img_rows, img_cols=img_cols, color_type=color_type,\n",
        "                             nb_dense_block=nb_dense_block, growth_rate=growth_rate,\n",
        "                             nb_filter=nb_filter, num_classes=num_classes)\n",
        "\n",
        "'''\n",
        "def distillation_loss(y_true, y_pred, teacher_pred, temperature=3.0, alpha=0.1):\n",
        "    \"\"\"\n",
        "    Compute the distillation loss combining both:\n",
        "    - Soft target loss (KL divergence)\n",
        "    - Hard target loss (standard cross-entropy)\n",
        "    \"\"\"\n",
        "    soft_labels = K.softmax(teacher_pred / temperature)\n",
        "    soft_student = K.softmax(y_pred / temperature)\n",
        "\n",
        "    # KL divergence for the soft labels\n",
        "    distillation_loss = K.mean(K.sum(soft_labels * K.log(soft_labels / (soft_student + 1e-6)), axis=-1))\n",
        "\n",
        "    # Standard cross-entropy loss\n",
        "    standard_loss = K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Weighted sum of the losses\n",
        "    return alpha * distillation_loss + (1. - alpha) * standard_loss\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fo3AgO1_X_Fw"
      },
      "outputs": [],
      "source": [
        "# Cell 2: Define DenseNet-121 model and helper functions\n",
        "from keras.layers import Concatenate, Conv2D, Activation, BatchNormalization, Dropout, ZeroPadding2D, AveragePooling2D, GlobalAveragePooling2D, Dense, MaxPooling2D, Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "def densenet121_model(img_rows, img_cols, color_type=1, nb_dense_block=4, growth_rate=32, nb_filter=64, reduction=0.5, dropout_rate=0.0, weight_decay=1e-4, num_classes=None):\n",
        "    global concat_axis\n",
        "    img_input = Input(shape=(img_rows, img_cols, color_type), name='data')\n",
        "    concat_axis = 3\n",
        "\n",
        "    nb_filter = 64\n",
        "    nb_layers = [6, 12, 24, 16]\n",
        "\n",
        "    x = Conv2D(nb_filter, (7, 7), strides=(2, 2), name='conv1', use_bias=False)(img_input)\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
        "\n",
        "    for block_idx in range(nb_dense_block - 1):\n",
        "        stage = block_idx + 2\n",
        "        x, nb_filter = dense_block(x, stage, nb_layers[block_idx], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "        x = transition_block(x, stage, nb_filter, dropout_rate=dropout_rate)\n",
        "        nb_filter = int(nb_filter)\n",
        "\n",
        "    final_stage = stage + 1\n",
        "    x, nb_filter = dense_block(x, final_stage, nb_layers[-1], nb_filter, growth_rate, dropout_rate=dropout_rate)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x_fc = GlobalAveragePooling2D()(x)\n",
        "    x_fc = Dense(1000)(x_fc)\n",
        "    x_fc = Activation('softmax')(x_fc)\n",
        "\n",
        "    model = Model(img_input, x_fc)\n",
        "\n",
        "    x_newfc = GlobalAveragePooling2D()(x)\n",
        "    x_newfc = Dense(num_classes)(x_newfc)\n",
        "    x_newfc = Activation('softmax')(x_newfc)\n",
        "\n",
        "    model = Model(img_input, x_newfc)\n",
        "\n",
        "    sgd = SGD(learning_rate=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_block(x, stage, branch, nb_filter, dropout_rate=None):\n",
        "    inter_channel = nb_filter * 4\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(inter_channel, (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = ZeroPadding2D((1, 1))(x)\n",
        "    x = Conv2D(nb_filter, (3, 3), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def transition_block(x, stage, nb_filter, dropout_rate=None):\n",
        "    x = BatchNormalization(axis=concat_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(int(nb_filter), (1, 1), use_bias=False)(x)\n",
        "\n",
        "    if dropout_rate:\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    x = AveragePooling2D((2, 2), strides=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def dense_block(x, stage, nb_layers, nb_filter, growth_rate, dropout_rate=None, grow_nb_filters=True):\n",
        "    concat_feat = x\n",
        "\n",
        "    for i in range(nb_layers):\n",
        "        branch = i + 1\n",
        "        x = conv_block(concat_feat, stage, branch, growth_rate, dropout_rate)\n",
        "        concat_feat = Concatenate(axis=concat_axis)([concat_feat, x])\n",
        "\n",
        "        if grow_nb_filters:\n",
        "            nb_filter += growth_rate\n",
        "\n",
        "    return concat_feat, nb_filter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vkze25r2V7aE",
        "outputId": "fc4adae1-a012-490d-c6d0-125ddc3c95ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 4s/step - accuracy: 0.1855 - loss: 3.1519 - val_accuracy: 0.0548 - val_loss: 48.2657\n",
            "Epoch 2/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 5s/step - accuracy: 0.3331 - loss: 2.4529 - val_accuracy: 0.1024 - val_loss: 26.8962\n",
            "Epoch 3/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 5s/step - accuracy: 0.4138 - loss: 2.1162 - val_accuracy: 0.0833 - val_loss: 6.3118\n",
            "Epoch 4/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 5s/step - accuracy: 0.4938 - loss: 1.7455 - val_accuracy: 0.2190 - val_loss: 3.7254\n",
            "Epoch 5/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 5s/step - accuracy: 0.5365 - loss: 1.5111 - val_accuracy: 0.2810 - val_loss: 3.8692\n",
            "Epoch 6/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 5s/step - accuracy: 0.6047 - loss: 1.3345 - val_accuracy: 0.2143 - val_loss: 4.3160\n",
            "Epoch 7/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 4s/step - accuracy: 0.7000 - loss: 0.9600 - val_accuracy: 0.6071 - val_loss: 1.4104\n",
            "Epoch 8/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 4s/step - accuracy: 0.7276 - loss: 0.9209 - val_accuracy: 0.3381 - val_loss: 3.7573\n",
            "Epoch 9/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m501s\u001b[0m 4s/step - accuracy: 0.7184 - loss: 0.8947 - val_accuracy: 0.4143 - val_loss: 2.8056\n",
            "Epoch 10/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 5s/step - accuracy: 0.8021 - loss: 0.6941 - val_accuracy: 0.3000 - val_loss: 4.1632\n",
            "Epoch 11/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m503s\u001b[0m 5s/step - accuracy: 0.7969 - loss: 0.5701 - val_accuracy: 0.2095 - val_loss: 12.0618\n",
            "Epoch 12/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 4s/step - accuracy: 0.8514 - loss: 0.5045 - val_accuracy: 0.4952 - val_loss: 2.7795\n",
            "Epoch 13/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m519s\u001b[0m 5s/step - accuracy: 0.8655 - loss: 0.4194 - val_accuracy: 0.6024 - val_loss: 1.5827\n",
            "Epoch 14/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 4s/step - accuracy: 0.9128 - loss: 0.2660 - val_accuracy: 0.4119 - val_loss: 3.2393\n",
            "Epoch 15/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 4s/step - accuracy: 0.8980 - loss: 0.3002 - val_accuracy: 0.5571 - val_loss: 2.0744\n",
            "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 2s/step\n",
            "Epoch 1/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 2s/step - accuracy: 0.0395 - loss: 3.0446 - val_accuracy: 0.0429 - val_loss: 3.0445\n",
            "Epoch 2/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 2s/step - accuracy: 0.0464 - loss: 3.0446 - val_accuracy: 0.0429 - val_loss: 3.0445\n",
            "Epoch 3/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.0436 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0445\n",
            "Epoch 4/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.0415 - loss: 3.0446 - val_accuracy: 0.0405 - val_loss: 3.0445\n",
            "Epoch 5/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.0466 - loss: 3.0446 - val_accuracy: 0.0357 - val_loss: 3.0445\n",
            "Epoch 6/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.0363 - loss: 3.0446 - val_accuracy: 0.0500 - val_loss: 3.0446\n",
            "Epoch 7/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 2s/step - accuracy: 0.0387 - loss: 3.0446 - val_accuracy: 0.0524 - val_loss: 3.0446\n",
            "Epoch 8/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.0440 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n",
            "Epoch 9/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 2s/step - accuracy: 0.0392 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n",
            "Epoch 10/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 2s/step - accuracy: 0.0517 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n",
            "Epoch 11/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 2s/step - accuracy: 0.0505 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n",
            "Epoch 12/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 2s/step - accuracy: 0.0568 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n",
            "Epoch 13/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.0485 - loss: 3.0446 - val_accuracy: 0.0429 - val_loss: 3.0446\n",
            "Epoch 14/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2s/step - accuracy: 0.0422 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n",
            "Epoch 15/15\n",
            "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 2s/step - accuracy: 0.0455 - loss: 3.0446 - val_accuracy: 0.0452 - val_loss: 3.0446\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fdaf6e5c3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "'''\n",
        "import tensorflow as tf\n",
        "# Build the teacher and student models\n",
        "teacher = densenet121_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)  # 21 classes in UC-Merced\n",
        "student = student_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)\n",
        "\n",
        "teacher.compile(optimizer=SGD(learning_rate=1e-2, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Teacher model should be pre-trained (for simplicity, using it as untrained here)\n",
        "# for layer in teacher.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "# # Compile the student model with the distillation loss\n",
        "# student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "#                 loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher.predict(x_train)),\n",
        "#                 metrics=['accuracy'])\n",
        "\n",
        "# # Train the student model\n",
        "# student.fit(x_train, y_train,\n",
        "#             batch_size=16,\n",
        "#             epochs=5,\n",
        "#             validation_data=(x_test, y_test),\n",
        "#             shuffle=True)\n",
        "# Precompute teacher's predictions for the training set\n",
        "teacher_preds = teacher.predict(x_train)\n",
        "\n",
        "# Define distillation loss function\n",
        "# def distillation_loss(y_true, y_pred, teacher_preds, temperature=3):\n",
        "#     y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "#     y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "#     teacher_preds = tf.keras.activations.softmax(teacher_preds / temperature)\n",
        "\n",
        "#     # Cross-entropy between the student predictions and the teacher predictions\n",
        "#     return tf.keras.losses.categorical_crossentropy(teacher_preds, y_pred)\n",
        "\n",
        "\n",
        "'''\n",
        "# def distillation_loss(y_true, y_pred, teacher_preds, temperature=3):\n",
        "#     # Get the current batch size\n",
        "#     batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "#     # Use tf.gather to select the relevant teacher predictions for the current batch\n",
        "#     teacher_batch_preds = tf.gather(teacher_preds, tf.range(batch_size))\n",
        "\n",
        "#     y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "#     y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "#     teacher_batch_preds = tf.keras.activations.softmax(teacher_batch_preds / temperature)\n",
        "\n",
        "#     # Cross-entropy between the student predictions and the teacher predictions\n",
        "#     return tf.keras.losses.categorical_crossentropy(teacher_batch_preds, y_pred)\n",
        "'''\n",
        "def distillation_loss(y_true, y_pred, teacher_preds, temperature=3.0, alpha=0.1):\n",
        "    \"\"\"\n",
        "    Compute the distillation loss combining both:\n",
        "    - Soft target loss (KL divergence)\n",
        "    - Hard target loss (standard cross-entropy)\n",
        "    \"\"\"\n",
        "    # Get the current batch size\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    # Use tf.gather to select the relevant teacher predictions for the current batch\n",
        "    teacher_batch_preds = tf.gather(teacher_preds, tf.range(batch_size))\n",
        "\n",
        "    # Softmax for soft labels and student predictions with temperature scaling\n",
        "    y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "    y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "    teacher_batch_preds = tf.keras.activations.softmax(teacher_batch_preds / temperature)\n",
        "\n",
        "    # Cross-entropy between the student predictions and the teacher predictions (soft targets)\n",
        "    soft_loss = tf.keras.losses.categorical_crossentropy(teacher_batch_preds, y_pred)\n",
        "\n",
        "    # Standard cross-entropy loss with hard targets\n",
        "    hard_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Weighted sum of the distillation loss and the hard target loss\n",
        "    return alpha * soft_loss + (1. - alpha) * hard_loss\n",
        "\n",
        "\n",
        "# Compile the student model with the distillation loss (using precomputed teacher predictions)\n",
        "student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "                loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Train the student model\n",
        "student.fit(x_train, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=5,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "'''\n",
        "# Import required libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Assuming densenet121_model and student_model functions are already defined\n",
        "# Replace with appropriate model definitions if necessary\n",
        "\n",
        "# Build the teacher and student models\n",
        "teacher = densenet121_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)  # 21 classes in UC-Merced\n",
        "student = student_model(img_rows=128, img_cols=128, color_type=3, num_classes=21)\n",
        "\n",
        "# Compile the teacher model (pre-training the teacher model if necessary)\n",
        "teacher.compile(optimizer=SGD(learning_rate=1e-2, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train teacher model (pretrain if needed)\n",
        "# Note: If teacher model is already pre-trained, you can load weights here instead of training\n",
        "teacher.fit(x_train, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=15,  # Adjust epochs as needed for pre-training\n",
        "            validation_data=(x_test, y_test))\n",
        "\n",
        "# Precompute teacher's predictions for the training set\n",
        "teacher_preds = teacher.predict(x_train)\n",
        "\n",
        "# Define distillation loss function\n",
        "def distillation_loss(y_true, y_pred, teacher_preds, temperature=3.0, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Compute the distillation loss combining both:\n",
        "    - Soft target loss (KL divergence)\n",
        "    - Hard target loss (standard cross-entropy)\n",
        "    \"\"\"\n",
        "    # Get the current batch size\n",
        "    batch_size = tf.shape(y_pred)[0]\n",
        "\n",
        "    # Use tf.gather to select the relevant teacher predictions for the current batch\n",
        "    teacher_batch_preds = tf.gather(teacher_preds, tf.range(batch_size))\n",
        "\n",
        "    # Softmax for soft labels and student predictions with temperature scaling\n",
        "    y_true = tf.keras.activations.softmax(y_true / temperature)\n",
        "    y_pred = tf.keras.activations.softmax(y_pred / temperature)\n",
        "    teacher_batch_preds = tf.keras.activations.softmax(teacher_batch_preds / temperature)\n",
        "\n",
        "    # Cross-entropy between the student predictions and the teacher predictions (soft targets)\n",
        "    soft_loss = tf.keras.losses.categorical_crossentropy(teacher_batch_preds, y_pred)\n",
        "\n",
        "    # Standard cross-entropy loss with hard targets\n",
        "    hard_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "    # Weighted sum of the distillation loss and the hard target loss\n",
        "    return alpha * soft_loss + (1. - alpha) * hard_loss\n",
        "\n",
        "# Compile the student model with the distillation loss (using precomputed teacher predictions)\n",
        "student.compile(optimizer=SGD(learning_rate=1e-3, momentum=0.9),\n",
        "                loss=lambda y_true, y_pred: distillation_loss(y_true, y_pred, teacher_preds),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# Train the student model\n",
        "student.fit(x_train, y_train,\n",
        "            batch_size=16,\n",
        "            epochs=15,\n",
        "            validation_data=(x_test, y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VipkPiCJV8_J",
        "outputId": "a380c129-a733-4dd5-aea0-a78389796472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 4.52%\n"
          ]
        }
      ],
      "source": [
        " # Evaluate the trained student model\n",
        "score = student.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Test accuracy: {score[1] * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYnfWU1GWBnT",
        "outputId": "c29657cd-6bdd-4155-f3c6-e1bc69e1ff1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher test accuracy: 55.71%\n"
          ]
        }
      ],
      "source": [
        "teacher_score = teacher.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Teacher test accuracy: {teacher_score[1] * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poCzU3RBkdJs",
        "outputId": "7374fe95-fdc9-448e-c981-abcabee79676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Teacher Model Evaluation ===\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2s/step\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     agricultural       0.83      0.29      0.43        17\n",
            "         airplane       1.00      0.71      0.83        17\n",
            "  baseballdiamond       0.86      0.57      0.69        21\n",
            "            beach       1.00      0.15      0.26        20\n",
            "        buildings       0.83      0.29      0.43        17\n",
            "        chaparral       1.00      0.44      0.61        16\n",
            " denseresidential       0.14      0.06      0.08        18\n",
            "           forest       0.53      0.96      0.69        24\n",
            "          freeway       0.36      0.67      0.47        18\n",
            "       golfcourse       0.73      0.30      0.42        27\n",
            "           harbor       1.00      0.86      0.93        22\n",
            "     intersection       0.92      0.48      0.63        23\n",
            "mediumresidential       0.34      0.88      0.49        26\n",
            "   mobilehomepark       0.50      0.89      0.64        18\n",
            "         overpass       0.76      0.65      0.70        20\n",
            "       parkinglot       0.77      0.85      0.81        20\n",
            "            river       0.52      0.70      0.59        23\n",
            "           runway       0.33      0.13      0.19        15\n",
            "sparseresidential       0.75      0.43      0.55        14\n",
            "     storagetanks       0.34      0.94      0.50        18\n",
            "      tenniscourt       0.46      0.23      0.31        26\n",
            "\n",
            "         accuracy                           0.56       420\n",
            "        macro avg       0.67      0.55      0.54       420\n",
            "     weighted avg       0.66      0.56      0.54       420\n",
            "\n",
            "Inference Time: 41.1600 seconds\n",
            "\n",
            "=== Student Model Evaluation ===\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 700ms/step\n",
            "                   precision    recall  f1-score   support\n",
            "\n",
            "     agricultural       0.40      0.12      0.18        17\n",
            "         airplane       0.00      0.00      0.00        17\n",
            "  baseballdiamond       0.01      0.05      0.02        21\n",
            "            beach       0.00      0.00      0.00        20\n",
            "        buildings       0.00      0.00      0.00        17\n",
            "        chaparral       0.00      0.00      0.00        16\n",
            " denseresidential       0.00      0.00      0.00        18\n",
            "           forest       0.00      0.00      0.00        24\n",
            "          freeway       0.04      0.50      0.08        18\n",
            "       golfcourse       0.00      0.00      0.00        27\n",
            "           harbor       0.00      0.00      0.00        22\n",
            "     intersection       0.00      0.00      0.00        23\n",
            "mediumresidential       0.00      0.00      0.00        26\n",
            "   mobilehomepark       0.00      0.00      0.00        18\n",
            "         overpass       0.00      0.00      0.00        20\n",
            "       parkinglot       0.75      0.15      0.25        20\n",
            "            river       0.00      0.00      0.00        23\n",
            "           runway       0.00      0.00      0.00        15\n",
            "sparseresidential       0.00      0.00      0.00        14\n",
            "     storagetanks       0.11      0.22      0.15        18\n",
            "      tenniscourt       0.00      0.00      0.00        26\n",
            "\n",
            "         accuracy                           0.05       420\n",
            "        macro avg       0.06      0.05      0.03       420\n",
            "     weighted avg       0.06      0.05      0.03       420\n",
            "\n",
            "Inference Time: 13.4288 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Cell: Evaluate and print additional metrics for teacher and student models\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    # Predict the labels\n",
        "    start_time = time.time()\n",
        "    y_pred_probs = model.predict(x_test)\n",
        "    inference_time = time.time() - start_time\n",
        "\n",
        "    # Get the predicted classes\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate precision, recall, F1 score\n",
        "    report = classification_report(y_true, y_pred, target_names=lb.classes_)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(report)\n",
        "    print(f\"Inference Time: {inference_time:.4f} seconds\")\n",
        "    return y_pred_probs\n",
        "\n",
        "print(\"=== Teacher Model Evaluation ===\")\n",
        "teacher_pred_probs = evaluate_model(teacher, x_test, y_test)\n",
        "\n",
        "print(\"\\n=== Student Model Evaluation ===\")\n",
        "student_pred_probs = evaluate_model(student, x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJsQnbwR6Rj3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}